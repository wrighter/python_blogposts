{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14c2bf0",
   "metadata": {},
   "source": [
    "# Options to run pandas ```DataFrame.apply``` in parallel\n",
    "A common use case in pandas is to want to apply a function to rows in a `DataFrame`. For a novice, the temptation can be to iterate through the rows in the `DataFrame` and pass the data to a function, but that is not a good idea. (You can read [this article](https://www.wrighters.io/how-to-iterate-over-dataframe-rows-and-should-you/) for a detailed explanation of why). Pandas has a method on both `DataFrame`s and `Series` that applies a function to the data. Ideally, we want that to be a vectorized implementation. But in many cases a non-vectorized implementation already exists, or the solution cannot be vectorized. If the `DataFrame` is large enough, or the function slow enough, applying the function can be very time consuming. In those situations, a way to speed things up is to run the code in parallel on multiple CPUs. In this article, I'll survey a number of popular options for applying functions to pandas `DataFrame`s in parallel. \n",
    "\n",
    "## An example problem\n",
    "To make things more concrete, let's consider an example where each row in a `DataFrame` represents a sample of data. We want to calculate a value from each row. The calculation might be slow. For demonstration purposes, we'll just invent a CPU intensive task. It turns out calculating arctangent is one such task, so we'll just make a function that does a lot of that. Our data will be a simple `DataFrame` with one data point per row, but it will be randomized so that each row is likely to be unique. We want unique data so that optimization via caching or memoization doesn't impact our comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a9c0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e6cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our slow function\n",
    "def slow_function(start: float) -> float:\n",
    "    res = 0\n",
    "    for i in range(int(math.pow(start, 7))):\n",
    "        res += math.atan(i) * math.atan(i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661ba156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5 ms ± 465 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit slow_function(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ecb8e5",
   "metadata": {},
   "source": [
    "We can see that this function is fairly slow, so calculating it over hundreds of values will take multiple seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176f2033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>5.577242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>5.484517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>5.136881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>5.174797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>5.644561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        value\n",
       "495  5.577242\n",
       "496  5.484517\n",
       "497  5.136881\n",
       "498  5.174797\n",
       "499  5.644561"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sample data\n",
    "sample = pd.DataFrame({'value': np.random.random(500) + 5})\n",
    "sample.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f190f",
   "metadata": {},
   "source": [
    "## Running `apply`\n",
    "Now if we want to run our `slow_function` on each row, we can use `apply`. One quick note on `DataFrame.apply` - it will apply per column by default (it will use axis 0). This means the function will be invoked once per column. The applied function receives the column (a `Series`) each time it is called, not each row (also a `Series`). If we use `axis=1`, then `apply` will pass each row to the function instead. This is the choice what we want here.\n",
    "\n",
    "I'm using a `lambda` to pick out the `value` column to pass into the `slow_function`. At the end, I turn the resulting `Series` back into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3b21bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>414125.614960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>368264.399398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>232842.530062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>245144.830221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>450413.419081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             value\n",
       "495  414125.614960\n",
       "496  368264.399398\n",
       "497  232842.530062\n",
       "498  245144.830221\n",
       "499  450413.419081"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[-5:].apply(lambda r: slow_function(r['value']), axis=1).to_frame(name=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6140a2",
   "metadata": {},
   "source": [
    "That is a little ugly though. Wouldn't it be great if we could just use a vectorized solution on the entire column instead? Well it turns out there's a very easy way to create a vectorized solution using Numpy, just wrap it in `np.vectorize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c3d61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>414125.614960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>368264.399398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>232842.530062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>245144.830221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>450413.419081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             value\n",
       "495  414125.614960\n",
       "496  368264.399398\n",
       "497  232842.530062\n",
       "498  245144.830221\n",
       "499  450413.419081"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[-5:].apply(np.vectorize(slow_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df0b4d",
   "metadata": {},
   "source": [
    "But is this an _optimized_ vectorized solution? Unfortunately it's not. The docs for np.vectorize point this out:\n",
    "\n",
    "    The `vectorize` function is provided primarily for convenience, not for\n",
    "    performance. The implementation is essentially a for loop.\n",
    "    \n",
    "Let's verify the speeds here with some timings. We'll also just try running `apply` on the `value` column, which is a pandas `Series`. In this case, there's only one axis, so it applies the function to each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015583dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.5 s ± 426 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sample.apply(lambda r: slow_function(r['value']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82156796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.6 s ± 176 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sample.apply(np.vectorize(slow_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0630e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.7 s ± 130 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sample['value'].apply(slow_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4030a28",
   "metadata": {},
   "source": [
    "So all three of these methods are essentially doing the same thing. While the code for `np.vectorize` looks nice and clean, it's not faster. Each solution is running a `for` loop over each row in the `DataFrame` (or `Series`), running our `slow_function` on each value. So let's move on to the goal of this article: let's run this function on multiple cores at once.\n",
    "\n",
    "## Parallel processing\n",
    "Before we step into running our code on multiple cores, let's cover a few basics. Everything we've done thus far has all been done in one process. This means that the Python code is all running on one CPU core, even if my computer has multiple CPU cores available.\n",
    "\n",
    "If we want to take advantage of multiple processes or cores at once, we have that option in Python. The basic idea is to run multiple Python processes, and have each one perform a fraction of the calculations. Then all the results are returned to the primary process. For example, if we have 4 cores available, then we should be able to have each core perform 25% of the calculations at the same time. In theory, the job will be done 4 times faster. In reality, it will be less efficient than that.  \n",
    "\n",
    "## Comparing implementations\n",
    "Before we move on to parallel implementations, let's setup the code we'll use to compare them.\n",
    "\n",
    "Note that all of the code samples below are in one Python file (`slow_function.py`) for your convenience. You can use it to run the timings you'll see below, or run an any implementation from the command line. You can access it [here in my github repo](https://github.com/wrighter/python_blogposts/tree/main/performance/parallel/slow_function.py) and follow along in your own environment.\n",
    "\n",
    "To run this code, I created a clean virtualenv for this article using [pyenv](https://www.wrighters.io/use-pyenv-and-virtual-environments-to-manage-python-complexity/) and installed Python 3.9.12. All the projects were installed in the same virtualenv.\n",
    "\n",
    "For all of these code samples, we'll assume we have the following code is available:\n",
    "\n",
    "```python\n",
    "import math\n",
    "import sys\n",
    "import argparse\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def slow_function(start: float) -> float:\n",
    "    res = 0\n",
    "    for i in range(int(math.pow(start, 7))):\n",
    "        res += math.atan(i) * math.atan(i)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_sample():\n",
    "    data = {'value': np.random.random(500) + 5}\n",
    "    return data\n",
    "\n",
    "```\n",
    "\n",
    "Here is the default (single CPU) implementation, the same as what we ran above:\n",
    "\n",
    "```python\n",
    "def run_default():\n",
    "    import pandas as pd\n",
    "\n",
    "    sample = pd.DataFrame(get_sample())\n",
    "    sample['results'] = sample['value'].apply(slow_function)\n",
    "    print(\"Default results:\\n\", sample.tail(5))\n",
    "\n",
    "```\n",
    "\n",
    "My method for timing this is to run the `timeit` module on the code above, like this:\n",
    "```shell\n",
    "python -m timeit \"import slow_function; slow_function.run_default()\"\n",
    "```\n",
    "Which yields\n",
    "```\n",
    "1 loop, best of 5: 17.4 sec per loop\n",
    "```\n",
    "\n",
    "So our base problem is about 17 seconds to run. Can we improve on that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab4f6a",
   "metadata": {},
   "source": [
    "## Core `multiprocessing`\n",
    "As a base parallel case, we will implement a solution with the core Python `multiprocessing` module. Then we will look at a number of popular libraries that make this task easier to implement. You can decide which one is easiest to understand and use for your purposes. We'll also look at a few interesting tidbits about the projects that can help you make a decision on whether to use them.\n",
    "\n",
    "The [multiprocessing](https://docs.python.org/3/library/multiprocessing.html?#module-multiprocessing) module is fairly straightforward to use. It comes with core python, so there is no extra installation step. We only need to invoke it correctly. There are several ways to use the module, but I'll show you an example using `multiprocessing.Pool`.\n",
    "\n",
    "For more details on `multiprocessing`, you can [read my article that shows basic usage](https://www.wrighters.io/python-how-to-use-multiprocessing-to-finish-work-faster/).\n",
    "\n",
    "Note that `multiprocessing` doesn't know about pandas and `DataFrames`, so to send each row into the pool, we have to provide either the guts of our data, or an `iterable`. \n",
    "\n",
    "FYI: when using `multiprocessing`, you also might have to put your `slow_function` in a separate python file since the processes that are launched by the `multiprocessing` module have to have access to the same functions. This tends to show up on some platforms, like Windows or when running from Jupyter notebooks. In the case where I was running this code in a Jupyter notebook, I saw this error if using functions defined in the notebook: `AttributeError: Can't get attribute 'slow_function' on <module '__main__' (built-in)>`.\n",
    "\n",
    "This is what a multiprocessing implementaton looks like. \n",
    "```python\n",
    "def run_multiprocessing():\n",
    "    import pandas as pd\n",
    "\n",
    "    sample = pd.DataFrame(get_sample())\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.map(slow_function, sample['value'])\n",
    "    sample['results'] = results\n",
    "    print(\"Multiprocessing results:\\n\", sample.tail(5))\n",
    "```\n",
    "\n",
    "Again, running this using `timeit` as follows:\n",
    "```shell\n",
    "python -m timeit \"import slow_function; slow_function.run_multiprocessing()\"\n",
    "```\n",
    "\n",
    "produces\n",
    "```\n",
    "1 loop, best of 5: 5.86 sec per loop\n",
    "```\n",
    "\n",
    "Now we can see that the `multiprocessing` version runs a little more than 3x faster. My machine has 4 real cores (and 8 virtual cores), so this is somewhat in line with expectations. Instead of being 4x faster, it has to deal with a bit of overhead for copying the data and competing with other tasks on my machine. If we had even more cores available, we could further improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6954b27",
   "metadata": {},
   "source": [
    "## Other options\n",
    "Even with a simple example it's clear that using `multiprocessing` is not seamless. We have to extract the data from our `DataFrame` to pass into the `pool.map` function, and the results are returned in a list.  There's also a `__main__` guard boilerplate, and we had to move our function out to a separate file for Jupyter to work with it. \n",
    "\n",
    "There are a number of projects that build on top of multiprocessing, pandas, and other projects. Some of them even work directly with the concept of a `DataFrame`, but support distributed computing. For the rest of the article, we'll implement this simple problem using each project. This demonstrates how each one works and the basic steps to get it running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62946de8",
   "metadata": {},
   "source": [
    "## Joblib\n",
    "[Joblib](https://joblib.readthedocs.io/) is a generic set of tools for pipelining code in Python. It's not specifically integrated with pandas, but it's easy enough to use and has some other nice features such as disk caching of functions and memoization. \n",
    "\n",
    "You can install joblib with pip:\n",
    "```shell\n",
    "pip install joblib\n",
    "```\n",
    "\n",
    "The example code is fairly simple:\n",
    "```python\n",
    "def run_joblib():\n",
    "    import pandas as pd\n",
    "    from joblib import Parallel, delayed\n",
    "\n",
    "    sample = pd.DataFrame(get_sample())\n",
    "    results = Parallel(n_jobs=multiprocessing.cpu_count())(\n",
    "            delayed(slow_function)(i) for i in sample['value']\n",
    "            )\n",
    "    sample['results'] = results\n",
    "    print(\"joblib results:\\n\", sample.tail(5))\n",
    "```\n",
    "\n",
    "Checking the performance:\n",
    "```shell\n",
    "python -m timeit \"import slow_function; slow_function.run_joblib()\"\n",
    "```\n",
    "gives us\n",
    "```\n",
    "1 loop, best of 5: 5.77 sec per loop\n",
    "```\n",
    "\n",
    "For general parallel processing, `joblib` makes for cleaner code than the `multiprocessing`. The speed is the same, and the project offers some extra tools that can be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8234e002",
   "metadata": {},
   "source": [
    "Now we'll look a few projects that are more closely integrated with pandas. If you're used to working with pandas and look back at the code we've written so far, it might look a little clunky and different from other pandas `DataFrame` methods that you're used to. The rest of the projects will look quite a bit more like standard pandas code.\n",
    "\n",
    "## Dask\n",
    "[Dask](https://www.dask.org) is a library that scales the standard PyData tools, like pandas, NumPy, and scikit-learn. From a code perspective, it usually looks pretty similar to the code you are used to, but it's possible to scale out to multiple cores on one machine, or even clusters of multiple machines. Even though we are only looking at processing a `DataFrame` that will fit into memory on one machine, it's possible to run code with Dask that uses more memory than available on the main node.  But Dask work great with on your local machine and even provides benefits without a full cluster.\n",
    "\n",
    "As you see in the code below, a Dask `DataFrame` wraps a regular pandas `DataFrame`, and supplies a similar interface. The difference with Dask is that sometimes you need to supply some hints to the calculation (the `meta` argument to `apply`), and the execution is always deferred. To get the result, you call `compute`. But writing this code feels much the same as writing normal pandas code.\n",
    "\n",
    "You can install it using pip:\n",
    "```shell\n",
    "pip install \"dask[complete]\"\n",
    "```\n",
    "or if you're using conda:\n",
    "```shell\n",
    "conda install dask\n",
    "```\n",
    "\n",
    "This is a very basic intro, read the [introductory docs](https://www.dask.org/get-started) for more complete examples.\n",
    "\n",
    "In order for dask to run in parallel on a local host, you'll have to start a local cluster. We do this only once.\n",
    "\n",
    "```python\n",
    "\n",
    "# global variable\n",
    "DASK_RUNNING = False\n",
    "\n",
    "def run_dask():\n",
    "    import pandas as pd\n",
    "    import dask.dataframe as dd\n",
    "\n",
    "    # hack for allowing our timeit code to work with one cluster\n",
    "    global DASK_RUNNING\n",
    "\n",
    "    if not DASK_RUNNING:\n",
    "        # normally, you'll do this once in your code\n",
    "        from dask.distributed import Client, LocalCluster\n",
    "        cluster = LocalCluster()  # Launches a scheduler and workers locally\n",
    "        client = Client(cluster)  # Connect to distributed cluster and override default\n",
    "        print(f\"Started cluster at {cluster.dashboard_link}\")\n",
    "        DASK_RUNNING = True\n",
    "\n",
    "    sample = pd.DataFrame(get_sample())\n",
    "    \n",
    "    dask_sample = dd.from_pandas(sample, npartitions=multiprocessing.cpu_count())\n",
    "    dask_sample['results'] = dask_sample['value'].apply(slow_function, meta=('value', 'float64')).compute()\n",
    "\n",
    "    print(\"Dask results:\\n\", dask_sample.tail(5))\n",
    "\n",
    "```\n",
    "\n",
    "Again, we time this as follows:\n",
    "```\n",
    "python -m timeit \"import slow_function; slow_function.run_dask()\"\n",
    "```\n",
    "\n",
    "and get\n",
    "```\n",
    "1 loop, best of 5: 5.21 sec per loop\n",
    "```\n",
    "\n",
    "Note that when you are running a local cluster, you can access a handy dashboard for monitoring the cluster, it's available via the field `cluster.dashboard_link`.\n",
    "\n",
    "On my machine, Dask performs as well as the other parallel options. It has the added benefit of monitoring and further scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0258c424",
   "metadata": {},
   "source": [
    "## Modin\n",
    "[Modin](https://modin.readthedocs.io/en/latest/) is a library that is built on top of Dask (and other libraries) but serves as a drop in replacement for pandas, making it even easier to work with existing code. When using Modin, they suggest replacing the `import pandas as pd` line as `import modin.pandas as pd`. That may be the only change needed to take advantage of it. Modin will provide speed improvements out of the box, and with some configuration and use of other libraries, can continue to scale up. \n",
    "\n",
    "You install Modin with pip:\n",
    "```shell\n",
    "pip install modin\n",
    "```\n",
    "\n",
    "But you'll need to install a backend as well. See the section of the [docs](https://modin.readthedocs.io/en/stable/#installation-and-choosing-your-compute-engine) for more details. Since we just installed Dask above, I'll use that. I'll also run the Dask cluster for Modin to use.\n",
    "\n",
    "```shell\n",
    "pip install \"modin[dask]\"\n",
    "```\n",
    "\n",
    "Note that besides the imports and Dask setup, our code looks exactly like bare pandas code.\n",
    "\n",
    "```python\n",
    "def run_modin():\n",
    "    global DASK_RUNNING\n",
    "    \n",
    "    import os\n",
    "\n",
    "    os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "    if not DASK_RUNNING:\n",
    "        from dask.distributed import Client, LocalCluster\n",
    "        cluster = LocalCluster()  # Launches a scheduler and workers locally\n",
    "        client = Client(cluster)  # Connect to distributed cluster and override default\n",
    "        print(f\"Started cluster at {cluster.dashboard_link}\")\n",
    "        DASK_RUNNING = True\n",
    "\n",
    "    import modin.pandas as pd\n",
    "    sample = pd.DataFrame(get_sample())\n",
    "    sample['results'] = sample['value'].apply(slow_function)\n",
    "    print(\"Modin results:\\n\", sample.tail(5))\n",
    "\n",
    "```\n",
    "\n",
    "Timing from \n",
    "```shell\n",
    "python -m timeit \"import slow_function; slow_function.run_modin()\"\n",
    "```\n",
    "gives us\n",
    "```\n",
    "1 loop, best of 5: 5.57 sec per loop\n",
    "```\n",
    "\n",
    "Modin with Dask provides the benefits of Dask, without the code differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7f1f0",
   "metadata": {},
   "source": [
    "## Swifter\n",
    "[Swifter](https://github.com/jmcarpenter2/swifter) is a package that figures out the best way to apply a function to a pandas `DataFrame`. It can do several things, including multiprocessing and vectorization. It integrates with other libraries like Dask and Modin, and will attempt to use them in the most efficient way possible. To use it, you just use the Swifter version of `apply`, not the one from `DataFrame` - as shown below.\n",
    "\n",
    "You can install swifter with pip:\n",
    "```shell\n",
    "pip install swifter\n",
    "```\n",
    "To use it with Modin, just import modin before swifter (or register it with `swifter.register_modin()`). It's almost the same as the base pandas version.\n",
    "\n",
    "```python\n",
    "\n",
    "def run_swifter():\n",
    "    global DASK_RUNNING\n",
    "    \n",
    "    import os\n",
    "\n",
    "    os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "    if not DASK_RUNNING:\n",
    "        from dask.distributed import Client, LocalCluster\n",
    "        cluster = LocalCluster()  # Launches a scheduler and workers locally\n",
    "        client = Client(cluster)  # Connect to distributed cluster and override default\n",
    "        print(f\"Started cluster at {cluster.dashboard_link}\")\n",
    "        DASK_RUNNING = True\n",
    "\n",
    "    import pandas as pd\n",
    "    import swifter\n",
    "\n",
    "    swifter.register_modin() # or could import modin.pandas as pd\n",
    "\n",
    "    sample = pd.DataFrame(get_sample())\n",
    "    sample['results'] = sample['value'].swifter.apply(slow_function)\n",
    "    print(\"Swifter results:\\n\", sample.tail(5))\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Double-checking the performance:\n",
    "```shell\n",
    "python -m timeit \"import slow_function; slow_function.run_swifter()\"\n",
    "```\n",
    "gives us slightly slower results:\n",
    "```\n",
    "1 loop, best of 5: 12.3 sec per loop\n",
    "```\n",
    "\n",
    "While there is a speed difference (Swifter is slowere here), this can be explained by the fact that Swifter samples the data in order to determine whether it is worthwhile to use a parallel option. For larger calculations, this extra work will be negligible. Changing the defaults is very easy through configuration, see [docs](https://github.com/jmcarpenter2/swifter/blob/master/docs/documentation.md) for more details.\n",
    "\n",
    "Swifter also includes some handy progress bars in both the shell and Jupyter notebooks. For longer running jobs, that is very convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ccf11",
   "metadata": {},
   "source": [
    "## Pandarallel\n",
    "[Pandarallel](https://github.com/nalepae/pandarallel/) is another project that integrates with pandas, similar to Swifter. You need to do a small initialization, then use the extra `DataFrame` methods to apply a method to a `DataFrame` in parallel. It has nice support for Jupyter progress bars as well, which can be a nice touch for users running it in a notebook. It doesn't have the same level of support for distributed libraries like Dask. But it's very simple code to write.\n",
    "\n",
    "You install Pandarallel with pip:\n",
    "```shell\n",
    "pip install pandarallel\n",
    "```\n",
    "\n",
    "```python\n",
    "def run_pandarallel():\n",
    "    from pandarallel import pandarallel\n",
    "\n",
    "    pandarallel.initialize()\n",
    "\n",
    "    import pandas as pd\n",
    "    sample = pd.DataFrame(get_sample())\n",
    "    sample['results'] = sample['value'].parallel_apply(slow_function)\n",
    "    print(\"Pandarallel results:\\n\", sample.tail(5))\n",
    "\n",
    "```\n",
    "\n",
    "Checking results with\n",
    "```shell\n",
    "python -m timeit \"import slow_function; slow_function.run_pandarallel()\"\n",
    "```\n",
    "yields\n",
    "```\n",
    "1 loop, best of 5: 5.12 sec per loop\n",
    "```\n",
    "\n",
    "If you are only looking for a simple way to run `apply` in parallel, and don't need the other improvements of the other projects, it can be a good option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d5b2c",
   "metadata": {},
   "source": [
    "## PySpark\n",
    "[PySpark](https://spark.apache.org/docs/latest/api/python/index.html) is a Python interface to [Apache Spark](https://spark.apache.org). The Spark project is a multi-language engine for executing data engineering, data science, and machine learning tasks in a clustered environment. Similar to Dask, it can scale up from single machines to entire clusters. It also supports multiple languages.\n",
    "\n",
    "PySpark contains a pandas API, so it is possible to write pandas code that works on Spark with little effort. Note that the pandas API is not 100% complete and also has some minor differences from standard pandas. But as you'll see, there are performance impacts that might make porting code to PySpark worth it.\n",
    "\n",
    "You can install pyspark with pip (I also needed to install PyArrow):\n",
    "```shell\n",
    "pip install pyspark pyarrow\n",
    "```\n",
    "\n",
    "The sample code is similar to basic pandas.\n",
    "```python\n",
    "def run_pyspark():\n",
    "\n",
    "    import pyspark.pandas as ps\n",
    "\n",
    "    sample = ps.DataFrame(get_sample())\n",
    "    sample['results'] = sample['value'].apply(slow_function)\n",
    "    print(\"PySpark results:\\n\", sample.tail(5))\n",
    "```\n",
    "\n",
    "Testing the speed with\n",
    "```shell\n",
    "python -m timeit \"import slow_function; slow_function.run_pyspark()\"\n",
    "```\n",
    "gives\n",
    "```\n",
    "1 loop, best of 5: 2.73 sec per loop\n",
    "```\n",
    "\n",
    "This is quite a bit faster than the other options. But it's worth noting here that the underlying implementation is not running the same pandas code on more CPUs, but rather running the Spark code on multiple CPUs. This is just a simple example, and there is quite a bit of configuration possible with Spark, but you can see that pandas integration makes trying it out quite easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274e99b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "We looked at a simple CPU bound function that we applied to a `DataFrame` of data. This was our base case. We then used the following libraries to implement a parallel version:\n",
    "* multiprocessing\n",
    "* joblib\n",
    "* Dask\n",
    "* Modin\n",
    "* Swifter\n",
    "* Pandarallel\n",
    "* PySpark\n",
    "\n",
    "Each of these projects offers features and improvements over the base `multiprocessing` version, with improvements from 3 to 7 times over our base case. Depending on your needs, one of these projects can offer improved readability and scalability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
