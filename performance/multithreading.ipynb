{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'm going to show a few simple examples of how Python's ```multiprocessing``` module can be used to dramatically improve the performance of some tasks with python.\n",
    "\n",
    "It's very common to have some function or set of functions that you run in a loop. When you see a loop that's performing an expensive operation, you should immediately think of (at least) two ways to speed things up. The first is vectorization, which I won't cover here, and the second is multiple threads (or processes) to allow the work to be done concurrently.\n",
    "\n",
    "Let's look at a simple example of how the ```multiprocessing``` module in Python can be used to solve this problem. In ```multiprocessing```, multiple Python processes are created instead of multiple threads, bypassing the Global Interpreter Lock (GIL) that can significantly slow down threaded Python programs. The goal is to take pieces of work that can be subdivided, perform that work in different processes using the full resources of the computer, then returning the results of those calculations to the main process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import functools\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to create some dummy time series data. This could be any sort of time series, such as daily attendance, temperature, stock prices, or store sales. I'm using the ```itertools``` module to generate some sequential strings for labels. I'm also using pandas to make a business date range going back 10+ years. The idea here is to generate enough data of enough size that processing them will take some time that we can measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.bdate_range(\"20100101\", pd.Timestamp.today())\n",
    "labels = [\"\".join(l) for l in itertools.combinations(string.ascii_letters, 2)]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "for label in labels:\n",
    "    df = pd.Series(np.random.random(len(dates)), index=dates).to_frame(\"data\")\n",
    "    df.to_csv(os.path.join(\"data\", f\"{label}.csv\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we've got a chunk of data files. Let's just write a function that will read in one of those files, perform some simple calculations with the data, then return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(label):\n",
    "    path = os.path.join(\"data\", f\"{label}.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    return df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2969.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.496318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.288674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.248125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.494456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.743172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.999814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              data\n",
       "count  2969.000000\n",
       "mean      0.496318\n",
       "std       0.288674\n",
       "min       0.000338\n",
       "25%       0.248125\n",
       "50%       0.494456\n",
       "75%       0.743172\n",
       "max       0.999814"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_file('az')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the ```%timeit``` magic in our jupyter notebook or ipython session to see what the cost is to process a single file. The ```-o``` option will return a result that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.66 ms ± 299 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "r = %timeit -o process_file('az')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that processing these files in a single loop will be linear, so getting our expected run time (in seconds) is pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.498858340331422"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.average * len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since computers are fast, we'll go ahead and actually run this to see what the actual time is. Note that ```%timeit``` runs the code multiple times, so this can still take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.23 s ± 323 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit for l in labels: process_file(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not bad, on my computer I can process more than 1000 data files in only about 8 seconds. But if I have bigger files and need to do more complex calculations, I'll want to use more of my processing power. In my case, I have a quad core Intel i7 with 8 threads, your machine may be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.set_start_method('fork') # fix for MacOS bug: see https://github.com/ipython/ipython/issues/12396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before solving this problem using the ```multiprocessing``` module, let's look at a trivial example to see a few basic guidelines. First, all programs running ```multiprocessing``` need a guard to check if the process is the main process or a child process. This guard ensures that all the subprocesses can import the main code without side effects, such as trying to launch more processes in an endless loop. A second point is that you should avoid shared state between the processes and try to isolate the work in the function that you are executing. Finally, the arguments to the methods need to be pickleable, since that's how the module moves data between processes. Since we are loading a file from disk in the method and returning a small amount of data, this problem is a good candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main: <_MainProcess name='MainProcess' parent=None started>\n",
      "Child process: <Process name='Process-1' parent=81413 started>\n",
      "Hi\n"
     ]
    }
   ],
   "source": [
    "# our function that we will execute in another process\n",
    "def say_hi():\n",
    "    print(\"Child process:\", multiprocessing.current_process())\n",
    "    print('Hi')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    p = multiprocessing.Process(target=say_hi)\n",
    "    print(\"Main:\", multiprocessing.current_process())\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't want to just run one child process, but rather as many as we can effectively use on our computer. One good way to do this is to use a ```Pool```.  A ```Pool``` has multiple methods that can be used to execute functions. A simple and common method is to use ```map``` which is just a parallel version of the built-in method. It invokes the first argument method with each item in the iterable second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.94 s ± 75.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 2) as pool:\n",
    "        results = pool.map(process_file, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my machine, the speed improvement goes from about 8 seconds to a little over 2, about a 3.5x improvement. Obviously, as execution times get longer and less time is spent passing data back and forth between processes, this improvement will get closer to the number of processes available.\n",
    "\n",
    "One last thing that is worth looking at is a few examples of more complicated method invocations. What if instead of a simple list of single arguments, there were multiple arguments to the function? If the extra arguments are common, using ```functools.partial``` is a good solution. Just give the partial the extra arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file2(label, threshold):\n",
    "    path = os.path.join(\"data\", f\"{label}.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    return df['data'].mean() > threshold\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 2) as pool:\n",
    "        results = pool.map(functools.partial(process_file2, threshold=.2), labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also may have cases where the arguments are little more complicated and not fixed for all invocations. In this case, you can use ```starmap``` with a list of ```iterable``` arguments that get unpacked for each invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's just a simple example of data with different arguments for some of the labels\n",
    "def make_thresh(label):\n",
    "    if 'a' in label or 'z' in label:\n",
    "        return .3\n",
    "    else:\n",
    "        return .4\n",
    "    \n",
    "args = [(l, make_thresh(l)) for l in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count() - 2) as pool:\n",
    "        results = pool.starmap(process_file2, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this brief intro to the ```multiprocessing``` module has shown you some easy ways to speed up your Python code and make full use of your environment to finish work more quickly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
