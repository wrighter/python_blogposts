{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5960eeb4",
   "metadata": {},
   "source": [
    "# Using requests and BeautifulSoup in Python to scrape data\n",
    "The amount of data available on the internet is quite staggering. It is often quite easy to do a quick search and click through to view data on a website. However, if you want to actually use that data in your analysis, you have to be able to fetch it and convert it into a format that is usable. The creators and owners of the websites, however, may not want you do this. They might prefer that you only look at the data, along with its surrounding ads. The fact that you want to use the data for analysis inherently makes it valuable. The data provider most likely makes money from the ads you view as you look at the data, or they may even charge you for access to view the data itself, and so they are incentivized to stop you from fetching it. In this article, I'll show you a very basic way to download data when the simplest methods may not work. It will not work in every case, but you can add it to your toolbox to consider if you need to download some data.\n",
    "\n",
    "In a [previous article](https://www.wrighters.io/analyzing-stock-data-events-with-pandas/) I used the pandas library to download a table from Wikipedia. It worked quite well. Pandas will read an html page, look for tables within the page, then turns every table it finds into a list of `DataFrame`s for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606e8f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Fed. Funds Rate</th>\n",
       "      <th>Discount Rate</th>\n",
       "      <th>Votes</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>November 5, 2020</td>\n",
       "      <td>0%–0.25%</td>\n",
       "      <td>0.25%</td>\n",
       "      <td>10-0</td>\n",
       "      <td>Official statement</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>September 16, 2020</td>\n",
       "      <td>0%–0.25%</td>\n",
       "      <td>0.25%</td>\n",
       "      <td>8-2</td>\n",
       "      <td>Kaplan dissented, preferring \"the Committee [t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>August 27, 2020</td>\n",
       "      <td>0%–0.25%</td>\n",
       "      <td>0.25%</td>\n",
       "      <td>unanimous</td>\n",
       "      <td>No meeting, but announcement of approval of up...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>July 29, 2020</td>\n",
       "      <td>0%–0.25%</td>\n",
       "      <td>0.25%</td>\n",
       "      <td>10-0</td>\n",
       "      <td>Official statement</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>June 10, 2020</td>\n",
       "      <td>0%–0.25%</td>\n",
       "      <td>0.25%</td>\n",
       "      <td>10-0</td>\n",
       "      <td>Official statement</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date Fed. Funds Rate Discount Rate      Votes  \\\n",
       "0    November 5, 2020        0%–0.25%         0.25%       10-0   \n",
       "1  September 16, 2020        0%–0.25%         0.25%        8-2   \n",
       "2     August 27, 2020        0%–0.25%         0.25%  unanimous   \n",
       "3       July 29, 2020        0%–0.25%         0.25%       10-0   \n",
       "4       June 10, 2020        0%–0.25%         0.25%       10-0   \n",
       "\n",
       "                                               Notes  Unnamed: 5  \n",
       "0                                 Official statement         NaN  \n",
       "1  Kaplan dissented, preferring \"the Committee [t...         NaN  \n",
       "2  No meeting, but announcement of approval of up...         NaN  \n",
       "3                                 Official statement         NaN  \n",
       "4                                 Official statement         NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fomc = pd.read_html(\"https://en.wikipedia.org/wiki/History_of_Federal_Open_Market_Committee_actions\")\n",
    "\n",
    "print(len(fomc))\n",
    "fomc[1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b973387",
   "metadata": {},
   "source": [
    "## Why would we need anything else?\n",
    "If life was simple, this would work for all web pages we ever want to use. However, sometimes it just doesn't work, so we need to dig further into the details of how this works. For example, let's say we want to get historical earnings data from Yahoo! finance. I've talked about one of the Yahoo! finance APIs in [a previous article](https://www.wrighters.io/analyzing-intraday-and-overnight-stock-returns-with-pandas/), but the API I use there doesn't give you granular historical earnings data that is available on the Yahoo! finance earnings pages. Let's just see if we can grab historical earnings data for a symbol, like you will see [here, for AAPL](https://finance.yahoo.com/calendar/earnings/?symbol=AAPL). You can see this page in your browser, and it contains a table of results, but what happens when you try to load it using pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b5fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "url = \"https://finance.yahoo.com/calendar/earnings/?symbol=AAPL\"\n",
    "try:\n",
    "    pd.read_html(url)\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036fb33",
   "metadata": {},
   "source": [
    "## Why the 404?\n",
    "At the time of running this code, I got a 404 error. This means the page is \"not found\". But we know it does exist, so what is happening?\n",
    "\n",
    "This is Yahoo!'s way of telling you to buzz off, you are not welcome here with your screen scraping attempt. It turns out that Wikipedia allows us to download the web page mechanically, but Yahoo! doesn't. Can we perhaps still attempt to download the data?\n",
    "\n",
    "For web sites that return raw html for a request, it should be possible to read the data as long as you can convince the web server that you are not automated software, but a real web browser being read by a human being. If we look at the [source code](https://github.com/pandas-dev/pandas/blob/main/pandas/io/html.py) for `read_html`, we can see that basics of how pandas does this. The code is a bit complex, but feel free to read it over, but basically it does the following:\n",
    "1. fetches the raw html using `urllib`\n",
    "1. uses a parser to parse the raw html, then fetches all the tables\n",
    "1. turns the tables into `DataFrames`\n",
    "1. handles tons of options for all the above steps, including using different parsers and options for creating the `DataFrames`\n",
    "\n",
    "One thing that sticks out right away in the first step is that pandas doesn't set any HTTP headers (or allow you to pass them into this method), so Yahoo! is probably just rejecting that connection since it will look like it is automated. To write some lower level code, let's consider how we could connect directly to the Yahoo! HTTP server and have some more control over what we are sending in our request.\n",
    "\n",
    "## The requests library\n",
    "There is an easy to use python library called [requests](https://docs.python-requests.org/en/latest/) that can be used to automate HTTP requests. (If you want to dig into the HTTP specs, they are all [listed here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Resources_and_specifications)). Let's see what happens if we do the simplest request (an HTTP GET request) for the same url using requests. The requests library has methods for each of the HTTP verbs, and we can just pass it the url. It returns a response object that contains the wrapped server response. If you invoke `raise_for_status` on the response, it will raise an `HTTPError` for any error encountered. Install requests with `pip install requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "992354b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://finance.yahoo.com/calendar/earnings/?symbol=AAPL\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(url)\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b3cd7",
   "metadata": {},
   "source": [
    "OK, we have the same error, so that's a good start. What else could we do to try to convince Yahoo! we are a real browser? The `get` method is just a thin wrapper to the `request` method, which takes a number of parameters. One that is very important to consider is `headers`, a dict of HTTP headers to pass in on the request. Web browsers always send along an identifier called the [User-Agent](https://datatracker.ietf.org/doc/html/rfc7231#section-5.5.3). The most logical header to include first is a valid User-Agent. One way to get a value to use is to just see what your current web browser is sending. A handy way to do this is to use DuckDuckGo, it gives you this info when you ask it `my user agent`, like [this](http://duckduckgo.com/?q=my+user+agent).\n",
    "\n",
    "For me, this happened to be ```Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15```. Let's try adding that to our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92955253",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "                         \"AppleWebKit/605.1.15 (KHTML, like Gecko) \"\n",
    "                         \"Version/15.4 Safari/605.1.15\"}\n",
    "res = requests.get(url, headers=headers)\n",
    "try:\n",
    "    res.raise_for_status()\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5111c73",
   "metadata": {},
   "source": [
    "Now we have a valid response. What does it look like? The actual HTML that the browser renders is included in the `content` of the response. Let's just look at the beginning of it. It's just a standard html document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02ea723b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html><html data-color-theme=\"light\" id=\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.content[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647b454",
   "metadata": {},
   "source": [
    "## Now how do I read html?\n",
    "Your web browser takes this html and renders it into a nice looking web page, complete with ads, a table, colors, and styles applied to the visual elements. You just want to grab the raw data underneath. In order to do this, the html needs to be parsed. Instead of writing your own parser, you can use [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/), a library that parses the html and provides useful ways to extract what you want from it. Install it with `pip install beautifulsoup4`. You'll also want to install lxml - `pip install lxml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b5fa3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b739c",
   "metadata": {},
   "source": [
    "Now before we start to try to select the table and data out of the soup, it may be helpful to look at the page in your web browser, such as Firefox, Safari, or Chrome, and right click on the table and choose the \"inspect element\" or \"inpect\" option, assuming you have developer tools enabled. This will allow you to see the structure of the html document and the table itself.\n",
    "\n",
    "In this case, we only have one table (at the time of writing this, Yahoo! could always change things!), so we will try to select it out of the soup. The `select` method will return a list of all the `table` elements in the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe30fdb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.select(\"table\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c897eed1",
   "metadata": {},
   "source": [
    "Now that we've confirmed there's just one, let's see if we can get the header (`th`) and data rows (`tr`) from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d09bb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Symbol',\n",
       " 'Company',\n",
       " 'Earnings Date',\n",
       " 'EPS Estimate',\n",
       " 'Reported EPS',\n",
       " 'Surprise(%)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = soup.select(\"table\")[0]\n",
    "columns = []\n",
    "for th in table.select(\"th\"):\n",
    "    columns.append(th.text)\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001b11e6",
   "metadata": {},
   "source": [
    "Now let's grab the rows. We just loop through each table row (`tr`) and then each data element (`td`) and make a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6e76dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['AAPL', 'Apple Inc', 'Oct 26, 2022, 4 PMEDT', '-', '-', '-'],\n",
       " ['AAPL', 'Apple Inc.', 'Jan 15, 1997, 12 AMEST', '-0.02', '-0.03', '-48.31'],\n",
       " 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for tr in table.select(\"tr\"):\n",
    "    row = []\n",
    "    for td in tr.select(\"td\"):\n",
    "        row.append(td.text)\n",
    "    if len(row):\n",
    "        data.append(row)\n",
    "# first and last row and length of table\n",
    "data[0], data[-1], len(data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea5b044",
   "metadata": {},
   "source": [
    "We have the symbol, company name, a date that has a strange malformed timezone, and the Earnings Per Share (EPS) estimate and reported value, along with a percentage called the surprise. This is how high or low the earnings were compared to the estimate. Let's make a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176bf218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company</th>\n",
       "      <th>Earnings Date</th>\n",
       "      <th>EPS Estimate</th>\n",
       "      <th>Reported EPS</th>\n",
       "      <th>Surprise(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Oct 26, 2022, 4 PMEDT</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Jul 25, 2022, 4 PMEDT</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Apr 26, 2022, 4 PMEDT</td>\n",
       "      <td>1.43</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>Jan 27, 2022, 11 AMEST</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.1</td>\n",
       "      <td>+11.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>Oct 28, 2021, 12 PMEDT</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.24</td>\n",
       "      <td>+0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol     Company           Earnings Date EPS Estimate Reported EPS  \\\n",
       "0   AAPL   Apple Inc   Oct 26, 2022, 4 PMEDT            -            -   \n",
       "1   AAPL   Apple Inc   Jul 25, 2022, 4 PMEDT            -            -   \n",
       "2   AAPL   Apple Inc   Apr 26, 2022, 4 PMEDT         1.43            -   \n",
       "3   AAPL  Apple Inc.  Jan 27, 2022, 11 AMEST         1.89          2.1   \n",
       "4   AAPL  Apple Inc.  Oct 28, 2021, 12 PMEDT         1.24         1.24   \n",
       "\n",
       "  Surprise(%)  \n",
       "0           -  \n",
       "1           -  \n",
       "2           -  \n",
       "3      +11.17  \n",
       "4       +0.32  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79929f7b",
   "metadata": {},
   "source": [
    "## Data cleanup\n",
    "At this point, we just want to do a little data cleanup. Since everything is just text at this point, we need to first convert things to the correct data types. If you're interested in learning more about data conversion, you can check out [this article](https://www.wrighters.io/converting-types-in-pandas/). First, let's make all the numeric values be numbers, and if there's no data (like in the dates in the future), we'll set them to `NaN` by setting errors to `coerce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "212b4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['EPS Estimate', 'Reported EPS', 'Surprise(%)']:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d869e",
   "metadata": {},
   "source": [
    "Now the `Earnings Date` column is a little strange because it has a timezone aware datetime in it. I happen to know that AAPL always reports earnings after the market closes, so the historical *times* are not accurate, just the dates. But let's pretend we wanted to convert these into datetime objects anyway versus just dates. We need to turn this into a format that can be parsed properly by `pd.to_datetime`. What we have now doesn't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b78b6af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown string format: Oct 26, 2022, 4 PMEDT\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pd.to_datetime(df['Earnings Date'])\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec16423",
   "metadata": {},
   "source": [
    "Now ideally, we could just parse this by passing in a format (using [this nice reference](https://strftime.org)). I can attempt this with the AM/PM indicator and timezone connected, let's see if that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "385fe348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time data 'Oct 26, 2022, 4 PMEDT' does not match format '%b %d, %Y, %I %p%Z' (match)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pd.to_datetime(df['Earnings Date'], format='%b %d, %Y, %I %p%Z')\n",
    "except Exception as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee1223",
   "metadata": {},
   "source": [
    "The EDT/EST is not a full timezone name and doesn't get parsed by `to_datetime` (or even using `datetime.datetime.strptime` directly. Since we know the values are in the Eastern US timezone, we can just set it directly. We'll remove the timezone from the raw data, parse it into a datetime, then set the timezone.\n",
    "\n",
    "Note that I use the `str` accessor to do the string replace operation since that field starts off as a string. Then, when it's converted to a `datetime` using `pd.to_datetime`, I use the `dt` accessor to do the timezone localization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ffe6e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company</th>\n",
       "      <th>Earnings Date</th>\n",
       "      <th>EPS Estimate</th>\n",
       "      <th>Reported EPS</th>\n",
       "      <th>Surprise(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>2022-10-26 16:00:00-04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>2022-07-25 16:00:00-04:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>2022-04-26 16:00:00-04:00</td>\n",
       "      <td>1.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>2022-01-27 11:00:00-05:00</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.10</td>\n",
       "      <td>11.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>2021-10-28 12:00:00-04:00</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol     Company             Earnings Date  EPS Estimate  Reported EPS  \\\n",
       "0   AAPL   Apple Inc 2022-10-26 16:00:00-04:00           NaN           NaN   \n",
       "1   AAPL   Apple Inc 2022-07-25 16:00:00-04:00           NaN           NaN   \n",
       "2   AAPL   Apple Inc 2022-04-26 16:00:00-04:00          1.43           NaN   \n",
       "3   AAPL  Apple Inc. 2022-01-27 11:00:00-05:00          1.89          2.10   \n",
       "4   AAPL  Apple Inc. 2021-10-28 12:00:00-04:00          1.24          1.24   \n",
       "\n",
       "   Surprise(%)  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3        11.17  \n",
       "4         0.32  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the timezone part of the date\n",
    "df['Earnings Date'] = df['Earnings Date'].str.replace(\"EDT|EST\", \"\", regex=True)\n",
    "df['Earnings Date'] = pd.to_datetime(df['Earnings Date'])\n",
    "\n",
    "# set the timezone manually\n",
    "import pytz\n",
    "eastern = pytz.timezone('US/Eastern')\n",
    "df['Earnings Date'] = df['Earnings Date'].dt.tz_localize(eastern)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db223b25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this example, we first tried getting data from a web page using pandas, then used the requests library to get the data instead, after setting the `User-Agent` header. We then used BeautifulSoup to parse the html to extract a table. Finally, we cleaned up the data using pandas.\n",
    "\n",
    "At this point it makes sense to give you a few warnings. First, this method will not work on many websites. In this case, Yahoo! is only blocking obvious attempts to download data using automated software. This techique will not work for sites that do any of the following:\n",
    "1. Require authentication. You will need to authenticate your requests, which may or may not be easy to do, depending on the authentication method.\n",
    "1. Uses JavaScript for rendering. If a site is rendered in JavaScript, your requests will just return raw JavaScript code and the table (and data) will not be present in the response. There are ways to scrape data on JavaScript sites, but they are much more complicated and involve running a browser instance.\n",
    "1. Aggressively block automated code. Sites may choose to block any User-Agents that don't look like a real browser. There are many ways that sites can do this, and so you may find this technique doesn't work.\n",
    "\n",
    "On top of all of the above, you should be a good network citizen and not download aggressively from a website, even if they don't block you. Most sites are built and sized to handle typical human users who traverse a site slowly. You should, at a minimum, access the site as a human would. This means accessing it slowly, with lengthy pauses between fetching data.\n",
    "\n",
    "Finally, it's important to note that websites will change their formats, designs, and layouts frequently, which will probably break any code you write that downloads data. For this reason, critical code and infrastructure should rely on supported APIs.\n",
    "\n",
    "Hopefully you now have a better understanding of how data can be retrieved, parsed, and cleaned up from a basic website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
